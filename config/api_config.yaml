
gemini-2.0-flash-lite-001:
    model: gemini-2.0-flash-lite-001
    endpoints: null
    api_type: vertex
    parallel: 4
    project_id: research-su-llm-routing
    regions: us-central1

gemini-2.0-flash-001:
    model: gemini-2.0-flash-001
    endpoints: null
    api_type: vertex
    parallel: 4
    project_id: research-su-llm-routing
    regions: us-central1

gemini-2.0-flash-lite:
    model: gemini-2.0-flash-lite
    endpoints: null
    api_type: vertex
    parallel: 4
    project_id: research-su-llm-routing
    regions: us-central1

gemini-2.0-flash:
    model: gemini-2.0-flash
    endpoints: null
    api_type: vertex
    parallel: 4
    project_id: research-su-llm-routing
    regions: us-central1

gemini-2.5-flash:
    model: gemini-2.5-flash
    endpoints: null
    api_type: vertex
    parallel: 4
    project_id: research-su-llm-routing
    regions: us-central1


gemini-2.5-pro:
    model: gemini-2.5-pro
    endpoints: null
    api_type: vertex
    parallel: 4
    project_id: research-su-llm-routing
    regions: us-central1


meta/llama-3.3-70b-instruct-maas:
    model: meta/llama-3.3-70b-instruct-maas
    endpoints: null
    api_type: vertex
    parallel: 4
    project_id: research-su-llm-routing
    regions: us-central1
    max_tokens: 4096
    temperature: 0.0

meta/llama-4-maverick-17b-128e-instruct-maas:
    model: meta/llama-4-maverick-17b-128e-instruct-maas
    endpoints: null
    api_type: vertex
    parallel: 4
    project_id: research-su-llm-routing
    regions: us-east5
    max_tokens: 4096
    temperature: 0.0

meta/llama-4-scout-17b-16e-instruct-maas:
    model: meta/llama-4-scout-17b-16e-instruct-maas
    endpoints: null
    api_type: vertex
    parallel: 4
    project_id: research-su-llm-routing
    regions: us-east5
    max_tokens: 4096
    temperature: 0.0

claude-3-7-sonnet@20250219:
    model: claude-3-7-sonnet@20250219
    endpoints: null
    api_type: vertex
    parallel: 4
    project_id: research-su-llm-routing
    regions: europe-west1
    max_tokens: 4096
    temperature: 0.0

claude-sonnet-4@20250514:
    model: claude-sonnet-4@20250514
    endpoints: null
    api_type: vertex
    parallel: 4
    project_id: research-su-llm-routing
    regions: europe-west1
    max_tokens: 4096
    temperature: 0.0

claude-3-5-haiku@20241022:
    model: claude-3-5-haiku@20241022
    endpoints: null
    api_type: vertex
    parallel: 4
    project_id: research-su-llm-routing
    regions: us-east5
    max_tokens: 4096
    temperature: 0.0

# claude-opus-4@20250514:
#     model: claude-opus-4@20250514
#     endpoints: null
#     api_type: vertex
#     parallel: 4
#     project_id: research-su-llm-routing
#     regions: europe-west1
#     max_tokens: 4096
#     temperature: 0.0

mistral-large-2411:
    model: mistral-large-2411
    endpoints: null
    api_type: vertex
    parallel: 4
    project_id: research-su-llm-routing
    regions: us-central1
    max_tokens: 4096
    temperature: 0.0

mistral-small-2503:
    model: mistral-small-2503
    endpoints: null
    api_type: vertex
    parallel: 4
    project_id: research-su-llm-routing
    regions: us-central1
    max_tokens: 4096
    temperature: 0.0



# claude-3-7-sonnet-20250219-thinking-16k:
#     model: claude-3-7-sonnet-20250219
#     endpoints: null
#     max_tokens: 20000
#     budget_tokens: 16000
#     api_type: anthropic_thinking
#     parallel: 32

# deepseek-r1:
#     endpoints:
#         - api_key: <your_api_key>
#     api_type: deepseek_reasoner
#     parallel: 32

# claude-3-5-sonnet-20241022:
#     model: claude-3-5-sonnet-20241022
#     endpoints: null
#     max_tokens: 20000
#     api_type: anthropic
#     parallel: 32

# o3-mini-2025-01-31-high:
#     model: o3-mini-2025-01-31
#     endpoints: null
#     reasoning_effort: high
#     api_type: openai_thinking
#     parallel: 32

# gpt-4o-mini-2024-07-18:
#     model: gpt-4o-mini
#     endpoints: null
#     api_type: openai
#     parallel: 128
#     max_tokens: 8196
#     temperature: 0.0

# # Local inference examples
# qwq-32b:
#     model: Qwen/QwQ-32B
#     endpoints: null
#     api_type: sglang
#     local_engine: True
#     temperature: 0.6
#     end_think_token: "</think>"
#     max_tokens: 32000
    
# gemma-3-27b-it:
#     model: google/gemma-3-27b-it
#     endpoints:
#         - api_base: http://0.0.0.0:<port_number>/v1
#           api_key: '-'
#     api_type: openai
#     parallel: 128
#     max_tokens: 8196
#     temperature: 0.0


# #******** AWS NOVA MODEL *****************#
# aws_nova_light_v1:
#     model: aws_nova_light_v1
#     model_id: us.amazon.nova-lite-v1:0
#     endpoints: null
#     api_type: aws_nova
#     parallel: 8 
#     max_tokens: 4096
#     temperature: 0.0
    
